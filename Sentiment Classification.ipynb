{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Imports\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Loading\n",
    "data = pd.read_csv('train.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path = ['nltk_data']\n",
    "\n",
    "#Creating a set of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating all the functions required for cleaning data: \n",
    "# Lower casing, appostrophe Lookup, removing links, parsing hashtags, parsing mentions\n",
    "\n",
    "APPOSTROPHES = {\n",
    "    \"don't\"   : \"do not\",\n",
    "    \"won't\"   : \"will not\",\n",
    "    \"it's\"    : \"it is\",\n",
    "    \"can't\"   : \"can not\",\n",
    "    \"i'll\"    : \"i will\",\n",
    "    \"i've\"    : \"i have\",\n",
    "    \"you're\"  : \"you are\",\n",
    "    \"didn't\"  : \"did not\",\n",
    "    \"she's\"   : \"she is\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"we're\"   : \"we are\",\n",
    "    \"you've\"  : \"you have\",\n",
    "    \"aren't\"  : \"are not\",\n",
    "    \"she'd\"   : \"she would\",\n",
    "    \"he'd\"    : \"he would\",\n",
    "    \"let's\"   : \"let us\",\n",
    "    \"we've\"   : \"we have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"who's\"   : \"who is\",\n",
    "    \"i'd\"     : \"i would\",\n",
    "    \"i'm\"     : \"i am\",\n",
    "    \"you'll\"  : \"you will\",\n",
    "    \"isn't\"   : \"is not\",\n",
    "    \"that's\"  : \"that is\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"there's\" : \"there is\",\n",
    "    \"we'll\"   : \"we will\",\n",
    "    \"dont\"    : \"do not\"\n",
    "    }\n",
    "\n",
    "\n",
    "def appostropheLookup(tweet):\n",
    "    words = tweet.split()\n",
    "    reformed = [APPOSTROPHES[word] if word in APPOSTROPHES else word for word in words]\n",
    "    reformed = \" \".join(reformed)\n",
    "    return reformed\n",
    "\n",
    "\n",
    "def removeLinks(tweet):\n",
    "    links_re = re.compile(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\")\n",
    "    tweet = links_re.sub(\" \",tweet)\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    return tweet\n",
    "\n",
    "\n",
    "\n",
    "# Returns a list of common english terms (words)\n",
    "def initialize_words():\n",
    "    content = None\n",
    "    with open('words_alpha.txt') as f: # A file containing common english words\n",
    "        content = f.readlines()\n",
    "    return [word.rstrip('\\n') for word in content]\n",
    "\n",
    "\n",
    "def parse_sentence(sentence, wordlist):\n",
    "    new_sentence = \"\" # output    \n",
    "    terms = sentence.split()    \n",
    "    for term in terms:\n",
    "        if term[0] == '#': # this is a hashtag, parse it\n",
    "            new_sentence += parse_tag(term, wordlist)\n",
    "        else: # Just append the word\n",
    "            new_sentence += term\n",
    "        new_sentence += \" \"\n",
    "\n",
    "    return new_sentence \n",
    "\n",
    "\n",
    "def parse_tag(term, wordlist):\n",
    "    words = []\n",
    "    # Remove hashtag, split by dash\n",
    "    tags = term[1:].split('-')\n",
    "    for tag in tags:\n",
    "        word = find_word(tag, wordlist)    \n",
    "        while word != None and len(tag) > 0:\n",
    "            words.append(word)            \n",
    "            if len(tag) == len(word): # Special case for when eating rest of word\n",
    "                break\n",
    "            tag = tag[len(word):]\n",
    "            word = find_word(tag, wordlist)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def find_word(token, wordlist):\n",
    "    i = len(token) + 1\n",
    "    while i > 1:\n",
    "        i -= 1\n",
    "        if token[:i] in wordlist:\n",
    "            return token[:i]\n",
    "    return None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a preprocessor function to call all the the individual functions for cleaning the data\n",
    "\n",
    "def preprocess(tweet):\n",
    "    clean_tweet = tweet\n",
    "    \n",
    "    clean_tweet = removeLinks(clean_tweet)\n",
    "    \n",
    "    wordlist = initialize_words()\n",
    "    \n",
    "    clean_tweet = parse_sentence(clean_tweet, wordlist)\n",
    "    \n",
    "    mention_finder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    clean_tweet = mention_finder.sub(\"@MENTION\", clean_tweet)\n",
    "    \n",
    "    clean_tweet = clean_tweet.lower()\n",
    "     \n",
    "    clean_tweet = appostropheLookup(clean_tweet)\n",
    "    \n",
    "    #clean_tweet = clean_tweet.decode(\"utf8\").encode('ascii', 'ignore')\n",
    "    \n",
    "    return clean_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text): \n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem(doc):\n",
    "    return (stemmer.stem(w) for w in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    preprocessor = preprocess,\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words = en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Model\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
    "X_train = train['SentimentText'].values\n",
    "X_test = test['SentimentText'].values\n",
    "y_train = train['Sentiment']\n",
    "y_test = test['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2),\n",
       "        preprocessor=<function preproc...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(vectorizer, SVC(probability=True,\n",
    "                            kernel=\"linear\", class_weight=\"balanced\"))\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.734062469208789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"Terrible experience @HomeDepot Hollywood for kitchen remodel, with multiple problems and delays! No sense of urgency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"I had a good experience at @HomeDepot yesterday. Amazing customer service\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"OK OK experience at @HomeDepot \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"Rude staff \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"FINE experience at @HomeDepot \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"fine experience at @HomeDepot \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"fine \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"moderate excellent bad experience at @HomeDepot \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"friends are leaving me 'cause of this stupid love  http://bit.ly/ZoxZC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"Thanks, I need all the help i can get.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"@haugern The servers are now backup, if you experience any more problems then please let me know  Sorry about the delay...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97112116, 0.02887884]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict_proba([\"I wanted to sleep in this morning but a mean kid through a popsicle stick at me head.I wish I could fly away like those squirrels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1038232723570212864', 'name': 'AlokitaG', 'text': '@BrarRohin : Worst and pathetic service ever, I ordered headset and it is not working. I have been trying to be inâ€¦ https://t.co/0OOXxlLN6c'}\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import datetime\n",
    "from datetime import *\n",
    "\n",
    "consumerKey = ''\n",
    "consumerSecret = ''\n",
    "accessToken = ''\n",
    "accessTokenSecret = ''\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "startDate=(datetime.now().replace(microsecond=0)) + timedelta(hours=4)\n",
    "endDate =   startDate-timedelta(minutes=5)\n",
    "api = tweepy.API(auth)\n",
    "list_tweets = []\n",
    "for mentions in tweepy.Cursor(api.mentions_timeline).items():\n",
    "    if mentions.created_at>endDate and  mentions.created_at <startDate:\n",
    "        tweet = {'id': mentions.user.id_str, 'name': mentions.user.screen_name, 'text':mentions.text }\n",
    "        print(tweet)\n",
    "        list_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in list_tweets:\n",
    "    prediction = pipe.predict([item['text']])\n",
    "    if 1 in prediction:\n",
    "        api.update_status(\"@\" + item['name'] + \" Thank you for your feedback. We look forward to serve you again soon. Meanwhile, please follow the link to help us serve you better. https://bit.ly/2N36dgd <3\", in_reply_to_status_id = item['id'])\n",
    "    elif 0 in prediction:\n",
    "        api.update_status(\"@\" + item['name'] + \" We are terribly sorry to hear your experience with us. We value you and our customer support team will try to resolve the isssue as soon as possible. Meanwhile, please follow the link to help us serve you better. https://bit.ly/2N36dgd\", in_reply_to_status_id = item['id'])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
